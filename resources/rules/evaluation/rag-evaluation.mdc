---
description: Requires all new RAG implementations to include a synthetic test set and evaluation metrics (Context Relevancy, Faithfulness).
globs: RAG/**/*.py, RAG/**/*.ipynb, **/*rag*.py
alwaysApply: false
---

# RAG Evaluation Mandate

Any new Retrieval-Augmented Generation pipeline MUST include an evaluation component. A RAG pipeline is NOT considered complete without measurable retrieval and generation quality scores.

## Requirements

1. **Generate a synthetic test set** from the source documents using the pattern in `resources/skills/ai/data/synthetic-data/SKILL.md`. Default to `gemini-2.5-flash` for question generation.
2. **Measure at least two metrics:**
   - **Context Relevancy:** Did we retrieve the right chunks for the query?
   - **Faithfulness:** Is the generated answer grounded in the retrieved context, or is it hallucinated?
3. **Preferred evaluation frameworks:** DeepEval or Giskard AI, as demonstrated in `RAG/evaluation/`.

## Evaluation Workflow

```
Source Documents
      |
      v
Synthetic Test Set (gemini-2.5-flash via synthetic-data skill)
      |
      v
Run RAG Pipeline on each question
      |
      v
Score with DeepEval: Faithfulness + Context Relevancy
      |
      v
Report: per-question scores + aggregate pass/fail
```

## Minimum Passing Thresholds

| Metric | Minimum Score |
|--------|--------------|
| Context Relevancy | 0.7 |
| Faithfulness | 0.8 |

If scores fall below these thresholds, review chunking strategy, embedding model, and `similarity_top_k` before considering the pipeline production-ready.

## Skill References

- `resources/skills/ai/data/synthetic-data/SKILL.md` for test set generation
- `resources/skills/ai/retrieval/crag/SKILL.md` if retrieval quality is unreliable and web fallback is needed
